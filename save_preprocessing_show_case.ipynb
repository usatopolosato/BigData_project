{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7771ae4d-092e-4565-8b82-3f4b3d1f8445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK СЕССИЯ СОЗДАНА ДЛЯ ОБРАБОТКИ 31 МЛН СТРОК\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "НАЧАЛО ЗАГРУЗКИ ДАННЫХ ИЗ output.csv\n",
      "================================================================================\n",
      "✓ УСПЕШНО ЗАГРУЖЕНО: 31,013,853 строк\n",
      "✓ КОЛИЧЕСТВО СТОЛБЦОВ: 13\n",
      "✓ РАЗМЕР ДАННЫХ В ПАМЯТИ: ~3.00 GB (примерно)\n",
      "\n",
      "================================================================================\n",
      "СОЗДАНИЕ ПРИЗНАКОВ И ПРЕОБРАЗОВАНИЯ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ПРИМЕНЕНИЕ ФИЛЬТРОВ ДЛЯ ОЧИСТКИ ДАННЫХ\n",
      "================================================================================\n",
      "Начальное количество строк: 31,013,853\n",
      "✓ После фильтрации: 17,137,026 строк\n",
      "✓ Удалено строк: 13,876,827 (44.74%)\n",
      "\n",
      "================================================================================\n",
      "ДОПОЛНИТЕЛЬНАЯ ОЧИСТКА И ЗАПОЛНЕНИЕ ДАННЫХ\n",
      "================================================================================\n",
      "Заполнение пропущенных станций...\n",
      "Применение дополнительных фильтров...\n",
      "✓ ФИНАЛЬНОЕ КОЛИЧЕСТВО ДАННЫХ: 16,774,028 строк\n",
      "✓ ОБЩАЯ ОЧИСТКА: 45.91% удалено\n",
      "\n",
      "================================================================================\n",
      "СОЗДАНИЕ ВИТРИН ДАННЫХ ДЛЯ UNIT-ЭКОНОМИКИ\n",
      "================================================================================\n",
      "\n",
      "Создание: Основные метрики поездок...\n",
      "✓ Успешно сохранено: Основные метрики поездок\n",
      "\n",
      "Создание: Пользовательская активность...\n",
      "✓ Успешно сохранено: Пользовательская активность\n",
      "\n",
      "Создание: Использование велосипедов...\n",
      "✓ Успешно сохранено: Использование велосипедов\n",
      "\n",
      "Создание: Временные паттерны...\n",
      "✓ Успешно сохранено: Временные паттерны\n",
      "\n",
      "Создание: Географическая активность...\n",
      "✓ Успешно сохранено: Географическая активность\n",
      "\n",
      "Создание: Станционная экономика...\n",
      "✓ Успешно сохранено: Станционная экономика\n",
      "\n",
      "Создание: Метрики скорости...\n",
      "✓ Успешно сохранено: Метрики скорости\n",
      "\n",
      "Создание: Экономика возвратов...\n",
      "✓ Успешно сохранено: Экономика возвратов\n",
      "\n",
      "Создание: Агрегированные KPI...\n",
      "✓ Успешно сохранено: Агрегированные KPI\n",
      "\n",
      "Создание: Детализированные данные...\n",
      "✓ Успешно сохранено: Детализированные данные\n",
      "\n",
      "================================================================================\n",
      "СОХРАНЕНИЕ ПОЛНОГО НАБОРА ДАННЫХ\n",
      "================================================================================\n",
      "ПОЛНЫЙ НАБОР ДАННЫХ СОХРАНЕН: data/full_dataset.csv\n",
      "\n",
      "================================================================================\n",
      "ВСЕ ВИТРИНЫ УСПЕШНО СОЗДАНЫ И СОХРАНЕНЫ!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek, hour\n",
    "from pyspark.sql.functions import round, count, mean, min, max, stddev, countDistinct, col, when, datediff, desc, avg\n",
    "from pyspark.sql import Window\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivvyBikes_Save_AV\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"128m\") \\\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128m\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1800\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .config(\"spark.kryoserializer.buffer\", \"128m\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \n",
    "           \"-XX:+UseG1GC -XX:+UseCompressedOops -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "           \"-XX:+UseG1GC -XX:+UseCompressedOops -XX:MaxGCPauseMillis=200\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .config(\"spark.locality.wait\", \"0\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .config(\"spark.hadoop.parquet.enable.summary-metadata\", \"false\") \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", \"false\") \\\n",
    "    .config(\"spark.sql.parquet.filterPushdown\", \"true\") \\\n",
    "    .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\") \\\n",
    "    .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n",
    "    .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.output.fileoutputformat.compress\", \"true\") \\\n",
    "    .config(\"spark.hadoop.mapreduce.output.fileoutputformat.compress.codec\", \"org.apache.hadoop.io.compress.GzipCodec\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Установим уровень логирования\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPARK СЕССИЯ СОЗДАНА ДЛЯ ОБРАБОТКИ 31 МЛН СТРОК\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# СХЕМА ДАННЫХ\n",
    "# ============================================\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), True),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", DoubleType(), True),\n",
    "    StructField(\"start_lng\", DoubleType(), True),\n",
    "    StructField(\"end_lat\", DoubleType(), True),\n",
    "    StructField(\"end_lng\", DoubleType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ============================================\n",
    "# ЗАГРУЗКА ДАННЫХ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"НАЧАЛО ЗАГРУЗКИ ДАННЫХ ИЗ output.csv\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Добавляем больше опций для обработки больших файлов\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "        .option(\"encoding\", \"UTF-8\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"maxColumns\", \"200\") \\\n",
    "        .option(\"nullValue\", \"\") \\\n",
    "        .option(\"nanValue\", \"\") \\\n",
    "        .csv(\"output.csv\")\n",
    "    \n",
    "    # Проверяем загрузку\n",
    "    initial_count = df.count()\n",
    "    print(f\"✓ УСПЕШНО ЗАГРУЖЕНО: {initial_count:,} строк\")\n",
    "    print(f\"✓ КОЛИЧЕСТВО СТОЛБЦОВ: {len(df.columns)}\")\n",
    "    print(f\"✓ РАЗМЕР ДАННЫХ В ПАМЯТИ: ~{initial_count * len(df.columns) * 8 / (1024**3):.2f} GB (примерно)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ ОШИБКА ПРИ ЗАГРУЗКЕ: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# ============================================\n",
    "# ПРЕДВАРИТЕЛЬНАЯ ОПТИМИЗАЦИЯ\n",
    "# ============================================\n",
    "\n",
    "# Перераспределяем данные для лучшей параллельности\n",
    "df = df.repartition(200)\n",
    "\n",
    "# ============================================\n",
    "# Создание признаков\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОЗДАНИЕ ПРИЗНАКОВ И ПРЕОБРАЗОВАНИЯ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_with_features = df.withColumn(\"ride_date\", to_date(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_year\", year(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_month\", month(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_day\", dayofmonth(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_dayofweek\", dayofweek(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_hour\", hour(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_duration_seconds\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\"))) \\\n",
    "    .withColumn(\"ride_duration_minutes\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")) / 60.0)\n",
    "\n",
    "# Функция для расчета расстояния (формула Гаверсинуса)\n",
    "def haversine_distance_spark(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Расчет расстояния с использованием функций Spark\"\"\"\n",
    "    R = 6371  # Радиус Земли в км\n",
    "    \n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = sin(dlat/2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Добавляем географические признаки\n",
    "df_with_geo_features = df_with_features \\\n",
    "    .withColumn(\"distance_km\", haversine_distance_spark(\n",
    "        col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\"))) \\\n",
    "    .withColumn(\"lat_grid\", round(col(\"start_lat\"), 2)) \\\n",
    "    .withColumn(\"lng_grid\", round(col(\"start_lng\"), 2)) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "                when((col(\"ride_dayofweek\") == 1) | \n",
    "                     (col(\"ride_dayofweek\") == 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"lat_diff\", abs(col(\"end_lat\") - col(\"start_lat\"))) \\\n",
    "    .withColumn(\"lng_diff\", abs(col(\"end_lng\") - col(\"start_lng\"))) \\\n",
    "    .withColumn(\"coord_diff\", col(\"lat_diff\") + col(\"lng_diff\")) \\\n",
    "    .withColumn(\"same_station\", \n",
    "                when(col(\"start_station_name\") == col(\"end_station_name\"), True).otherwise(False))\n",
    "\n",
    "# ============================================\n",
    "# ФИЛЬТРАЦИЯ ДАННЫХ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРИМЕНЕНИЕ ФИЛЬТРОВ ДЛЯ ОЧИСТКИ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Сначала делаем базовые проверки\n",
    "initial_count = df_with_geo_features.count()\n",
    "print(f\"Начальное количество строк: {initial_count:,}\")\n",
    "\n",
    "# Применяем фильтры\n",
    "filtered_simple = df_with_geo_features.filter(\n",
    "    # 1. Базовая валидация\n",
    "    (col(\"ride_id\").isNotNull()) &\n",
    "    (col(\"started_at\").isNotNull()) &\n",
    "    (col(\"ended_at\").isNotNull()) &\n",
    "    (col(\"member_casual\").isNotNull()) &\n",
    "    (col(\"rideable_type\").isNotNull()) &\n",
    "    \n",
    "    # 2. Даты\n",
    "    (col(\"ended_at\") > col(\"started_at\")) &\n",
    "    \n",
    "    # 3. Длительность\n",
    "    (col(\"ride_duration_minutes\") >= 0) &\n",
    "    (col(\"ride_duration_minutes\") <= 1440) &\n",
    "    \n",
    "    # 4. Координаты (Чикаго)\n",
    "    (col(\"start_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"start_lng\").between(-87.95, -87.5)) &\n",
    "    (col(\"end_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"end_lng\").between(-87.95, -87.5)) &\n",
    "    \n",
    "    # 5. Не нулевые координаты\n",
    "    (col(\"start_lat\") != 0) &\n",
    "    (col(\"start_lng\") != 0) &\n",
    "    (col(\"end_lat\") != 0) &\n",
    "    (col(\"end_lng\") != 0) &\n",
    "    \n",
    "    # 6. Не тестовые станции\n",
    "    (~lower(col(\"start_station_name\")).contains(\"test\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"hubbard\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"watson\")) &\n",
    "    (~lower(col(\"end_station_name\")).contains(\"test\")) &\n",
    "    (~lower(col(\"end_station_name\")).contains(\"hubbard\")) &\n",
    "    (~lower(col(\"end_station_name\")).contains(\"watson\"))\n",
    ")\n",
    "\n",
    "# Удаляем дубликаты\n",
    "filtered_simple = filtered_simple.dropDuplicates([\"ride_id\"])\n",
    "\n",
    "# Проверяем результат фильтрации\n",
    "filtered_count = filtered_simple.count()\n",
    "removed_count = initial_count - filtered_count\n",
    "print(f\"✓ После фильтрации: {filtered_count:,} строк\")\n",
    "print(f\"✓ Удалено строк: {removed_count:,} ({removed_count/initial_count*100:.2f}%)\")\n",
    "\n",
    "# ============================================\n",
    "# ДОПОЛНИТЕЛЬНАЯ ОЧИСТКА\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ДОПОЛНИТЕЛЬНАЯ ОЧИСТКА И ЗАПОЛНЕНИЕ ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Создаем гео-хеши\n",
    "filtered_df = filtered_simple.withColumn(\n",
    "    \"start_geo_hash\", \n",
    "    concat(round(col(\"start_lat\"), 3).cast(\"string\"), lit(\"_\"), \n",
    "           round(col(\"start_lng\"), 3).cast(\"string\"))\n",
    ").withColumn(\n",
    "    \"end_geo_hash\", \n",
    "    concat(round(col(\"end_lat\"), 3).cast(\"string\"), lit(\"_\"), \n",
    "           round(col(\"end_lng\"), 3).cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Функция для заполнения пропущенных станций\n",
    "def fill_missing_stations(df, station_col, geo_hash_col):\n",
    "    \"\"\"Заполняет пропущенные названия станций наиболее частыми в той же геозоне\"\"\"\n",
    "    \n",
    "    # Кэшируем промежуточные данные\n",
    "    window_spec = Window.partitionBy(geo_hash_col).orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Создаем DataFrame с наиболее частыми станциями\n",
    "    common_stations = df.filter(col(station_col).isNotNull()) \\\n",
    "        .groupBy(geo_hash_col, station_col) \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(geo_hash_col, col(station_col).alias(f\"common_{station_col}\"))\n",
    "    \n",
    "    # Присоединяем наиболее частые станции\n",
    "    df_filled = df.join(broadcast(common_stations), on=geo_hash_col, how=\"left\") \\\n",
    "        .withColumn(f\"{station_col}_clean\", \n",
    "                   coalesce(col(station_col), col(f\"common_{station_col}\"))) \\\n",
    "        .drop(f\"common_{station_col}\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Заполняем пропущенные станции\n",
    "print(\"Заполнение пропущенных станций...\")\n",
    "filtered_df = fill_missing_stations(filtered_df, \"start_station_name\", \"start_geo_hash\")\n",
    "filtered_df = fill_missing_stations(filtered_df, \"end_station_name\", \"end_geo_hash\")\n",
    "\n",
    "# Заменяем оригинальные колонки\n",
    "filtered_df = filtered_df \\\n",
    "    .drop(\"start_station_name\", \"end_station_name\") \\\n",
    "    .withColumnRenamed(\"start_station_name_clean\", \"start_station_name\") \\\n",
    "    .withColumnRenamed(\"end_station_name_clean\", \"end_station_name\") \\\n",
    "    .drop(\"start_geo_hash\", \"end_geo_hash\")\n",
    "\n",
    "# Дополнительные фильтры для аномалий\n",
    "print(\"Применение дополнительных фильтров...\")\n",
    "filtered_df = filtered_df.filter(\n",
    "    # 1. Удаляем поездки с экстремально малым расстоянием при большой длительности\n",
    "    ~((col(\"distance_km\") < 0.01) & (col(\"ride_duration_minutes\") > 40)) &\n",
    "    \n",
    "    # 2. Фильтр для поездок на ту же станцию с аномальной длительностью\n",
    "    ~((col(\"start_station_name\") == col(\"end_station_name\")) & \n",
    "      (col(\"ride_duration_minutes\") > 1440))\n",
    ")\n",
    "\n",
    "# Добавляем скорость и фильтруем\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"speed_kmh\",\n",
    "    when(col(\"ride_duration_minutes\") > 0,\n",
    "         col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0)\n",
    ").filter((col(\"speed_kmh\") <= 45) | (col(\"speed_kmh\").isNull()))\n",
    "\n",
    "# Финальная очистка\n",
    "cleaned_df_with_features = filtered_df \\\n",
    "    .withColumn(\"speed_kmh\", \n",
    "                when(col(\"ride_duration_minutes\") > 0, \n",
    "                     col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0))\n",
    "\n",
    "# Кэшируем финальный датафрейм\n",
    "cleaned_df_with_features.cache()\n",
    "final_count = cleaned_df_with_features.count()\n",
    "print(f\"✓ ФИНАЛЬНОЕ КОЛИЧЕСТВО ДАННЫХ: {final_count:,} строк\")\n",
    "print(f\"✓ ОБЩАЯ ОЧИСТКА: {(initial_count - final_count)/initial_count*100:.2f}% удалено\")\n",
    "\n",
    "# ============================================\n",
    "# СОЗДАНИЕ ВИТРИН ДАННЫХ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОЗДАНИЕ ВИТРИН ДАННЫХ ДЛЯ UNIT-ЭКОНОМИКИ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Функция для оптимизированного сохранения\n",
    "def save_dataframe(df, path, name):\n",
    "    \"\"\"Сохраняет DataFrame с оптимизацией\"\"\"\n",
    "    print(f\"\\nСоздание: {name}...\")\n",
    "    \n",
    "    # Перераспределяем для записи\n",
    "    df = df.repartition(50)\n",
    "    \n",
    "    # Сохраняем с прогрессом\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .option(\"encoding\", \"UTF-8\") \\\n",
    "        .option(\"quoteAll\", \"true\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .csv(path)\n",
    "    \n",
    "    print(f\"✓ Успешно сохранено: {name}\")\n",
    "\n",
    "# Создаем папку для результатов\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Витрина 1: Основные метрики поездок\n",
    "ride_metrics = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"member_casual\",\n",
    "    \"rideable_type\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    \"same_station\",\n",
    "    \"start_station_name\",\n",
    "    \"end_station_name\",\n",
    "    col(\"ride_duration_minutes\").alias(\"unit_cost_driver\"),\n",
    "    col(\"distance_km\").alias(\"unit_distance_driver\"),\n",
    "    when(col(\"same_station\") == True, 1).otherwise(0).alias(\"is_return_trip\")\n",
    ")\n",
    "\n",
    "save_dataframe(ride_metrics, \"data/ride_metrics.csv\", \"Основные метрики поездок\")\n",
    "\n",
    "# Витрина 2: Пользовательская активность\n",
    "user_activity = cleaned_df_with_features.groupBy(\"member_casual\").agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    countDistinct(\"ride_id\").alias(\"unique_users_estimate\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"avg_ride_duration\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"avg_ride_distance\"),\n",
    "    round(stddev(\"ride_duration_minutes\"), 2).alias(\"std_ride_duration\"),\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 1).alias(\"total_usage_hours\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ")\n",
    "\n",
    "save_dataframe(user_activity, \"data/user_activity.csv\", \"Пользовательская активность\")\n",
    "\n",
    "# Витрина 3: Использование велосипедов\n",
    "bike_utilization = cleaned_df_with_features.groupBy(\"rideable_type\").agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 2).alias(\"market_share_percent\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"avg_usage_time_min\"),\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 1).alias(\"total_usage_hours\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"avg_distance_per_ride\"),\n",
    "    round(sum(\"distance_km\"), 1).alias(\"total_distance_km\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ").orderBy(desc(\"total_rides\"))\n",
    "\n",
    "save_dataframe(bike_utilization, \"data/bike_utilization.csv\", \"Использование велосипедов\")\n",
    "\n",
    "# Витрина 4: Временные паттерны\n",
    "time_patterns = cleaned_df_with_features.groupBy(\n",
    "    \"ride_year\", \"ride_month\", \"ride_dayofweek\", \"ride_hour\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"ride_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\"),\n",
    "    round(mean(when(col(\"is_weekend\") == True, 1).otherwise(0)) * 100, 1).alias(\"weekend_percent\")\n",
    ").orderBy(\"ride_year\", \"ride_month\", \"ride_dayofweek\", \"ride_hour\")\n",
    "\n",
    "save_dataframe(time_patterns, \"data/time_patterns.csv\", \"Временные паттерны\")\n",
    "\n",
    "# Витрина 5: Географическая активность\n",
    "geo_activity = cleaned_df_with_features.groupBy(\n",
    "    \"start_station_name\", \"end_station_name\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"route_frequency\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(stddev(\"ride_duration_minutes\"), 1).alias(\"std_duration\"),\n",
    "    round(sum(when(col(\"member_casual\") == \"member\", 1).otherwise(0))).alias(\"member_count\"),\n",
    "    round(sum(when(col(\"member_casual\") == \"casual\", 1).otherwise(0))).alias(\"casual_count\")\n",
    ").filter(col(\"route_frequency\") >= 10)\n",
    "\n",
    "save_dataframe(geo_activity, \"data/geo_activity.csv\", \"Географическая активность\")\n",
    "\n",
    "# Витрина 6: Станционная экономика\n",
    "station_economics = cleaned_df_with_features.groupBy(\"start_station_name\").agg(\n",
    "    count(\"*\").alias(\"departures_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_departure_duration\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_departure_distance\"),\n",
    "    countDistinct(\"end_station_name\").alias(\"unique_destinations\"),\n",
    "    round(sum(when(col(\"same_station\") == True, 1).otherwise(0))).alias(\"return_count\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ").filter(col(\"departures_count\") >= 5).orderBy(desc(\"departures_count\"))\n",
    "\n",
    "save_dataframe(station_economics, \"data/station_economics.csv\", \"Станционная экономика\")\n",
    "\n",
    "# Витрина 7: Метрики скорости\n",
    "speed_metrics = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"member_casual\",\n",
    "    \"rideable_type\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    when(col(\"speed_kmh\") > 25, \"high_speed\")\n",
    "    .when(col(\"speed_kmh\") > 15, \"medium_speed\")\n",
    "    .otherwise(\"low_speed\").alias(\"speed_category\"),\n",
    "    when(col(\"ride_duration_minutes\") > 120, \"long_ride\")\n",
    "    .when(col(\"ride_duration_minutes\") > 30, \"medium_ride\")\n",
    "    .otherwise(\"short_ride\").alias(\"duration_category\")\n",
    ").filter(col(\"speed_kmh\") > 0)\n",
    "\n",
    "save_dataframe(speed_metrics, \"data/speed_metrics.csv\", \"Метрики скорости\")\n",
    "\n",
    "# Витрина 8: Экономика возвратов\n",
    "return_economics = cleaned_df_with_features.groupBy(\n",
    "    \"start_station_name\", \n",
    "    col(\"same_station\").alias(\"is_return_trip\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\")\n",
    ").orderBy(\"start_station_name\", desc(\"is_return_trip\"))\n",
    "\n",
    "save_dataframe(return_economics, \"data/return_economics.csv\", \"Экономика возвратов\")\n",
    "\n",
    "# Витрина 9: Агрегированные KPI\n",
    "aggregated_kpi = cleaned_df_with_features.agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"casual\", 1).otherwise(0)) * 100, 1).alias(\"casual_percent\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_ride_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_ride_distance_km\"),\n",
    "    round(mean(\"speed_kmh\"), 1).alias(\"avg_speed_kmh\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\"),\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 0).alias(\"total_usage_hours\"),\n",
    "    countDistinct(\"start_station_name\").alias(\"unique_start_stations\"),\n",
    "    countDistinct(\"end_station_name\").alias(\"unique_end_stations\")\n",
    ")\n",
    "\n",
    "save_dataframe(aggregated_kpi, \"data/aggregated_kpi.csv\", \"Агрегированные KPI\")\n",
    "\n",
    "# Витрина 10: Детализированные данные\n",
    "detailed_data = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"rideable_type\",\n",
    "    \"member_casual\",\n",
    "    \"started_at\",\n",
    "    \"ended_at\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    \"same_station\",\n",
    "    \"is_weekend\",\n",
    "    \"ride_dayofweek\",\n",
    "    \"ride_hour\",\n",
    "    \"start_station_name\",\n",
    "    \"end_station_name\",\n",
    "    \"start_lat\",\n",
    "    \"start_lng\",\n",
    "    \"end_lat\",\n",
    "    \"end_lng\",\n",
    "    (col(\"ride_duration_minutes\") * 0.1).alias(\"estimated_wear_cost\"),\n",
    "    when(col(\"same_station\") == True, 5.0).otherwise(0.0).alias(\"estimated_rebalancing_saving\")\n",
    ")\n",
    "\n",
    "save_dataframe(detailed_data, \"data/detailed_data.csv\", \"Детализированные данные\")\n",
    "\n",
    "# Сохраняем оригинальные данные\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СОХРАНЕНИЕ ПОЛНОГО НАБОРА ДАННЫХ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cleaned_df_with_features.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"data/full_dataset.csv\")\n",
    "\n",
    "print(\"ПОЛНЫЙ НАБОР ДАННЫХ СОХРАНЕН: data/full_dataset.csv\")\n",
    "\n",
    "# Очистка кэша\n",
    "cleaned_df_with_features.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВСЕ ВИТРИНЫ УСПЕШНО СОЗДАНЫ И СОХРАНЕНЫ!\")\n",
    "print(\"=\"*80)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53abf4-e070-4372-8b1f-ca4165d52730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
