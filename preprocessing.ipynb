{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becd1f4b-8bb0-4daa-9bd8-2cf19e530000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek, hour\n",
    "from pyspark.sql.functions import countDistinct, avg, col\n",
    "from pyspark.sql import Window\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5af95a9-7fe2-48c7-8617-adcb9e4b5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivvyBikes_EDA\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c3b250-6196-4b3b-a356-47ac2529eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), True),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", DoubleType(), True),\n",
    "    StructField(\"start_lng\", DoubleType(), True),\n",
    "    StructField(\"end_lat\", DoubleType(), True),\n",
    "    StructField(\"end_lng\", DoubleType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Загружаем данные\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv(\"output.csv\")\n",
    "df_with_features = df.withColumn(\"ride_date\", to_date(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_year\", year(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_month\", month(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_day\", dayofmonth(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_dayofweek\", dayofweek(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_hour\", hour(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_duration_seconds\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\"))) \\\n",
    "    .withColumn(\"ride_duration_minutes\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")) / 60.0)\n",
    "\n",
    "# Функция для расчета расстояния (формула Гаверсинуса)\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Расчет расстояния между двумя точками в км\"\"\"\n",
    "    R = 6371  # Радиус Земли в км\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Регистрируем UDF\n",
    "haversine_udf = udf(haversine_distance, DoubleType())\n",
    "\n",
    "df_with_geo_features = df_with_features \\\n",
    "    .withColumn(\"distance_km\", \n",
    "                haversine_udf(col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\"))) \\\n",
    "    .withColumn(\"lat_grid\", round(col(\"start_lat\"), 2)) \\\n",
    "    .withColumn(\"lng_grid\", round(col(\"start_lng\"), 2)) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "                when((col(\"ride_dayofweek\") == 1) | \n",
    "                     (col(\"ride_dayofweek\") == 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"lat_diff\", abs(col(\"end_lat\") - col(\"start_lat\"))) \\\n",
    "    .withColumn(\"lng_diff\", abs(col(\"end_lng\") - col(\"start_lng\"))) \\\n",
    "    .withColumn(\"coord_diff\", col(\"lat_diff\") + col(\"lng_diff\")) \\\n",
    "    .withColumn(\"same_station\", \n",
    "                when(col(\"start_station_name\") == col(\"end_station_name\"), True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc503d0-cf64-41c9-965e-7d015e82503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Осталось: 18,455,642 записей\n"
     ]
    }
   ],
   "source": [
    "# Пробуем упрощенный подход без проверки скорости\n",
    "filtered_simple = df_with_geo_features.filter(\n",
    "    # 1. Базовая валидация\n",
    "    (col(\"ride_id\").isNotNull()) &\n",
    "    (col(\"started_at\").isNotNull()) &\n",
    "    (col(\"ended_at\").isNotNull()) &\n",
    "    (col(\"member_casual\").isNotNull()) &\n",
    "    (col(\"rideable_type\").isNotNull()) &\n",
    "    \n",
    "    # 2. Даты\n",
    "    (col(\"ended_at\") > col(\"started_at\")) &\n",
    "    \n",
    "    # 3. Длительность\n",
    "    (col(\"ride_duration_minutes\") >= 1) &\n",
    "    (col(\"ride_duration_minutes\") <= 1440) &\n",
    "    \n",
    "    # 4. Координаты\n",
    "    (col(\"start_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"start_lng\").between(-87.95, -87.5)) &\n",
    "    (col(\"end_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"end_lng\").between(-87.95, -87.5)) &\n",
    "    \n",
    "    # 5. Не нулевые координаты\n",
    "    (col(\"start_lat\") != 0) &\n",
    "    (col(\"start_lng\") != 0) &\n",
    "    \n",
    "    # 6. Не тестовые станции\n",
    "    (~lower(col(\"start_station_name\")).contains(\"test\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"hubbard\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"watson\"))\n",
    ")\n",
    "\n",
    "# Удаляем дубликаты\n",
    "filtered_simple = filtered_simple.dropDuplicates([\"ride_id\"])\n",
    "\n",
    "# Используем этот DataFrame для дальнейшей работы\n",
    "filtered_df = filtered_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5ada86-7140-476b-bbc6-aa6065dd0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.withColumn(\n",
    "    \"start_geo_hash\", \n",
    "    concat(round(col(\"start_lat\"), 3).cast(\"string\"), lit(\"_\"), round(col(\"start_lng\"), 3).cast(\"string\"))\n",
    ").withColumn(\n",
    "    \"end_geo_hash\", \n",
    "    concat(round(col(\"end_lat\"), 3).cast(\"string\"), lit(\"_\"), round(col(\"end_lng\"), 3).cast(\"string\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3cb30b0-5ca7-494a-9ce9-653c7a28d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для заполнения пропущенных станций\n",
    "def fill_missing_stations(df, station_col, geo_hash_col):\n",
    "    \"\"\"Заполняет пропущенные названия станций наиболее частыми в той же геозоне\"\"\"\n",
    "    \n",
    "    # Создаем оконную функцию для нахождения наиболее частой станции в геозоне\n",
    "    window_spec = Window.partitionBy(geo_hash_col).orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Создаем DataFrame с наиболее частыми станциями\n",
    "    common_stations = df.filter(col(station_col).isNotNull()) \\\n",
    "        .groupBy(geo_hash_col, station_col) \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(geo_hash_col, col(station_col).alias(f\"common_{station_col}\"))\n",
    "    \n",
    "    # Присоединяем наиболее частые станции\n",
    "    df_filled = df.join(common_stations, on=geo_hash_col, how=\"left\") \\\n",
    "        .withColumn(f\"{station_col}_clean\", \n",
    "                   coalesce(col(station_col), col(f\"common_{station_col}\"))) \\\n",
    "        .drop(f\"common_{station_col}\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Заполняем пропущенные стартовые станции\n",
    "filtered_df = fill_missing_stations(filtered_df, \"start_station_name\", \"start_geo_hash\")\n",
    "\n",
    "# Заполняем пропущенные конечные станции\n",
    "filtered_df = fill_missing_stations(filtered_df, \"end_station_name\", \"end_geo_hash\")\n",
    "\n",
    "# Заменяем оригинальные колонки очищенными\n",
    "filtered_df = filtered_df \\\n",
    "    .drop(\"start_station_name\", \"end_station_name\") \\\n",
    "    .withColumnRenamed(\"start_station_name_clean\", \"start_station_name\") \\\n",
    "    .withColumnRenamed(\"end_station_name_clean\", \"end_station_name\") \\\n",
    "    .drop(\"start_geo_hash\", \"end_geo_hash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5872aefb-fa73-4ddf-9d2a-e56e2a6a2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Удаляем поездки с экстремально малым расстоянием при большой длительности\n",
    "filtered_df = filtered_df.filter(\n",
    "    ~((col(\"distance_km\") < 0.01) & (col(\"ride_duration_minutes\") > 40))\n",
    ")\n",
    "\n",
    "# 2. Фильтр для поездок на ту же станцию с аномальной длительностью\n",
    "filtered_df = filtered_df.filter(\n",
    "    ~(\n",
    "        (col(\"start_station_name\") == col(\"end_station_name\")) &\n",
    "        (col(\"ride_duration_minutes\") > 1440)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Удаляем поездки с нереальной скоростью\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"speed_kmh\",\n",
    "    when(col(\"ride_duration_minutes\") > 0,\n",
    "         col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0)\n",
    ")\n",
    "\n",
    "filtered_df = filtered_df.filter(\n",
    "    (col(\"speed_kmh\") <= 45) | (col(\"speed_kmh\").isNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b1ffc1-b189-40b0-aeda-3fd9fdd6bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_with_features = filtered_df \\\n",
    "    .withColumn(\"lat_grid\", round(col(\"start_lat\"), 2)) \\\n",
    "    .withColumn(\"lng_grid\", round(col(\"start_lng\"), 2)) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "                when((col(\"ride_dayofweek\") == 1) | \n",
    "                     (col(\"ride_dayofweek\") == 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"lat_diff\", abs(col(\"end_lat\") - col(\"start_lat\"))) \\\n",
    "    .withColumn(\"lng_diff\", abs(col(\"end_lng\") - col(\"start_lng\"))) \\\n",
    "    .withColumn(\"coord_diff\", col(\"lat_diff\") + col(\"lng_diff\")) \\\n",
    "    .withColumn(\"speed_kmh\", \n",
    "                when(col(\"ride_duration_minutes\") > 0, \n",
    "                     col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7412cef2-b515-4b08-a85d-c1c07771d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. БАЗОВАЯ СТАТИСТИКА:\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o723.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 248) (192.168.1.67 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.evaluate(BatchEvalPythonExec.scala:83)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonEvaluatorFactory$EvalPythonPartitionEvaluator.eval(EvalPythonEvaluatorFactory.scala:113)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:77)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2$adapted(EvalPythonExec.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.evaluate(BatchEvalPythonExec.scala:83)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonEvaluatorFactory$EvalPythonPartitionEvaluator.eval(EvalPythonEvaluatorFactory.scala:113)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:77)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2$adapted(EvalPythonExec.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. БАЗОВАЯ СТАТИСТИКА:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m      7\u001b[0m basic_stats \u001b[38;5;241m=\u001b[39m cleaned_df_with_features\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m      8\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_records\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mround\u001b[39m(mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_duration\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_duration\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_duration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m )\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Всего записей: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasic_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_records\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Средняя длительность: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasic_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_duration\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m мин\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\sql\\classic\\dataframe.py:443\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m--> 443\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o723.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 248) (192.168.1.67 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.evaluate(BatchEvalPythonExec.scala:83)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonEvaluatorFactory$EvalPythonPartitionEvaluator.eval(EvalPythonEvaluatorFactory.scala:113)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:77)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2$adapted(EvalPythonExec.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonEvaluatorFactory.evaluate(BatchEvalPythonExec.scala:83)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonEvaluatorFactory$EvalPythonPartitionEvaluator.eval(EvalPythonEvaluatorFactory.scala:113)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:77)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2$adapted(EvalPythonExec.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# Разделяем проверку на несколько частей\n",
    "\n",
    "# Часть 1: Базовая статистика\n",
    "print(\"\\n1. БАЗОВАЯ СТАТИСТИКА:\")\n",
    "print()\n",
    "\n",
    "basic_stats = cleaned_df_with_features.select(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"avg_duration\"),\n",
    "    round(min(\"ride_duration_minutes\"), 2).alias(\"min_duration\"),\n",
    "    round(max(\"ride_duration_minutes\"), 2).alias(\"max_duration\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Всего записей: {basic_stats['total_records']:,}\")\n",
    "print(f\"  Средняя длительность: {basic_stats['avg_duration']} мин\")\n",
    "print(f\"  Минимальная: {basic_stats['min_duration']} мин\")\n",
    "print(f\"  Максимальная: {basic_stats['max_duration']} мин\")\n",
    "\n",
    "# Часть 2: Пропущенные значения\n",
    "print(\"\\n2. ПРОПУЩЕННЫЕ ЗНАЧЕНИЯ:\")\n",
    "print()\n",
    "\n",
    "missing_stats = cleaned_df_with_features.select(\n",
    "    count(when(col(\"start_station_name\").isNull(), 1)).alias(\"missing_start_stations\"),\n",
    "    count(when(col(\"end_station_name\").isNull(), 1)).alias(\"missing_end_stations\"),\n",
    "    count(when(col(\"start_lat\").isNull(), 1)).alias(\"missing_start_lat\"),\n",
    "    count(when(col(\"end_lat\").isNull(), 1)).alias(\"missing_end_lat\")\n",
    ").collect()[0]\n",
    "\n",
    "total_records = basic_stats['total_records']\n",
    "print(f\"  Стартовые станции: {missing_stats['missing_start_stations']:,} \"\n",
    "      f\"({missing_stats['missing_start_stations']/total_records*100:.4f}%)\")\n",
    "print(f\"  Конечные станции: {missing_stats['missing_end_stations']:,} \"\n",
    "      f\"({missing_stats['missing_end_stations']/total_records*100:.4f}%)\")\n",
    "\n",
    "# Часть 3: География\n",
    "print(\"\\n3. ГЕОГРАФИЧЕСКИЙ ДИАПАЗОН:\")\n",
    "print()\n",
    "\n",
    "geo_stats = cleaned_df_with_features.select(\n",
    "    round(min(\"start_lat\"), 2).alias(\"min_start_lat\"),\n",
    "    round(max(\"start_lat\"), 2).alias(\"max_start_lat\"),\n",
    "    round(min(\"start_lng\"), 2).alias(\"min_start_lng\"),\n",
    "    round(max(\"start_lng\"), 2).alias(\"max_start_lng\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Широта: [{geo_stats['min_start_lat']:.2f}, {geo_stats['max_start_lat']:.2f}]\")\n",
    "print(f\"  Долгота: [{geo_stats['min_start_lng']:.2f}, {geo_stats['max_start_lng']:.2f}]\")\n",
    "\n",
    "# Часть 4: Типы данных (быстрая проверка)\n",
    "print(\"\\n4. РАСПРЕДЕЛЕНИЕ ПО ТИПАМ:\")\n",
    "print()\n",
    "\n",
    "# Используем приближенную проверку через выборку\n",
    "sample_df = cleaned_df_with_features.sample(0.01)  # 1% выборка\n",
    "\n",
    "bike_types = sample_df.select(\"rideable_type\").distinct().collect()\n",
    "user_types = sample_df.select(\"member_casual\").distinct().collect()\n",
    "\n",
    "print(f\"  Типы велосипедов: {[row['rideable_type'] for row in bike_types]}\")\n",
    "print(f\"  Типы пользователей: {[row['member_casual'] for row in user_types]}\")\n",
    "\n",
    "# Часть 5: Дополнительная статистика по подвыборке\n",
    "print(\"\\n5. СТАТИСТИКА ПО ВЫБОРКЕ (1%):\")\n",
    "print()\n",
    "\n",
    "sample_stats = sample_df.select(\n",
    "    count(\"*\").alias(\"sample_size\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"avg_distance_sample\"),\n",
    "    round(stddev(\"distance_km\"), 3).alias(\"std_distance_sample\"),\n",
    "    round(mean(\"speed_kmh\"), 1).alias(\"avg_speed_sample\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Размер выборки: {sample_stats['sample_size']:,}\")\n",
    "print(f\"  Среднее расстояние: {sample_stats['avg_distance_sample']} км\")\n",
    "print(f\"  Средняя скорость: {sample_stats['avg_speed_sample']} км/ч\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21eea8-f0ee-4051-889a-0d0b6c2b0309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
