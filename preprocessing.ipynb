{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becd1f4b-8bb0-4daa-9bd8-2cf19e530000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek, hour\n",
    "from pyspark.sql.functions import round, count, mean, min, max, stddev, countDistinct, col, when, datediff, desc, avg\n",
    "from pyspark.sql import Window\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5af95a9-7fe2-48c7-8617-adcb9e4b5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivvyBikes_Preprocessing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c3b250-6196-4b3b-a356-47ac2529eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ride_id\", StringType(), True),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"started_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_name\", StringType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_name\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"start_lat\", DoubleType(), True),\n",
    "    StructField(\"start_lng\", DoubleType(), True),\n",
    "    StructField(\"end_lat\", DoubleType(), True),\n",
    "    StructField(\"end_lng\", DoubleType(), True),\n",
    "    StructField(\"member_casual\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Загружаем данные\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv(\"output.csv\")\n",
    "df_with_features = df.withColumn(\"ride_date\", to_date(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_year\", year(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_month\", month(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_day\", dayofmonth(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_dayofweek\", dayofweek(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_hour\", hour(col(\"started_at\"))) \\\n",
    "    .withColumn(\"ride_duration_seconds\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\"))) \\\n",
    "    .withColumn(\"ride_duration_minutes\", \n",
    "               (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")) / 60.0)\n",
    "\n",
    "# Функция для расчета расстояния (формула Гаверсинуса)\n",
    "from pyspark.sql.functions import radians, sin, cos, atan2, sqrt, asin, lit\n",
    "\n",
    "# Используем встроенные функции Spark вместо Python UDF\n",
    "def haversine_distance_spark(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Расчет расстояния с использованием функций Spark\"\"\"\n",
    "    R = 6371  # Радиус Земли в км\n",
    "    \n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = sin(dlat/2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Используем функцию напрямую без UDF\n",
    "df_with_geo_features = df_with_features \\\n",
    "    .withColumn(\"distance_km\", haversine_distance_spark(\n",
    "        col(\"start_lat\"), col(\"start_lng\"), col(\"end_lat\"), col(\"end_lng\")))\\\n",
    "    .withColumn(\"lat_grid\", round(col(\"start_lat\"), 2)) \\\n",
    "    .withColumn(\"lng_grid\", round(col(\"start_lng\"), 2)) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "                when((col(\"ride_dayofweek\") == 1) | \n",
    "                     (col(\"ride_dayofweek\") == 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"lat_diff\", abs(col(\"end_lat\") - col(\"start_lat\"))) \\\n",
    "    .withColumn(\"lng_diff\", abs(col(\"end_lng\") - col(\"start_lng\"))) \\\n",
    "    .withColumn(\"coord_diff\", col(\"lat_diff\") + col(\"lng_diff\")) \\\n",
    "    .withColumn(\"same_station\", \n",
    "                when(col(\"start_station_name\") == col(\"end_station_name\"), True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc503d0-cf64-41c9-965e-7d015e82503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пробуем упрощенный подход без проверки скорости\n",
    "filtered_simple = df_with_geo_features.filter(\n",
    "    # 1. Базовая валидация\n",
    "    (col(\"ride_id\").isNotNull()) &\n",
    "    (col(\"started_at\").isNotNull()) &\n",
    "    (col(\"ended_at\").isNotNull()) &\n",
    "    (col(\"member_casual\").isNotNull()) &\n",
    "    (col(\"rideable_type\").isNotNull()) &\n",
    "    \n",
    "    # 2. Даты\n",
    "    (col(\"ended_at\") > col(\"started_at\")) &\n",
    "    \n",
    "    # 3. Длительность\n",
    "    (col(\"ride_duration_minutes\") >= 0) &\n",
    "    (col(\"ride_duration_minutes\") <= 1440) &\n",
    "    \n",
    "    # 4. Координаты\n",
    "    (col(\"start_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"start_lng\").between(-87.95, -87.5)) &\n",
    "    (col(\"end_lat\").between(41.6, 42.1)) &\n",
    "    (col(\"end_lng\").between(-87.95, -87.5)) &\n",
    "    \n",
    "    # 5. Не нулевые координаты\n",
    "    (col(\"start_lat\") != 0) &\n",
    "    (col(\"start_lng\") != 0) &\n",
    "    \n",
    "    # 6. Не тестовые станции\n",
    "    (~lower(col(\"start_station_name\")).contains(\"test\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"hubbard\")) &\n",
    "    (~lower(col(\"start_station_name\")).contains(\"watson\"))\n",
    ")\n",
    "\n",
    "# Удаляем дубликаты\n",
    "filtered_simple = filtered_simple.dropDuplicates([\"ride_id\"])\n",
    "\n",
    "# Используем этот DataFrame для дальнейшей работы\n",
    "filtered_df = filtered_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5ada86-7140-476b-bbc6-aa6065dd0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.withColumn(\n",
    "    \"start_geo_hash\", \n",
    "    concat(round(col(\"start_lat\"), 3).cast(\"string\"), lit(\"_\"), round(col(\"start_lng\"), 3).cast(\"string\"))\n",
    ").withColumn(\n",
    "    \"end_geo_hash\", \n",
    "    concat(round(col(\"end_lat\"), 3).cast(\"string\"), lit(\"_\"), round(col(\"end_lng\"), 3).cast(\"string\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3cb30b0-5ca7-494a-9ce9-653c7a28d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для заполнения пропущенных станций\n",
    "def fill_missing_stations(df, station_col, geo_hash_col):\n",
    "    \"\"\"Заполняет пропущенные названия станций наиболее частыми в той же геозоне\"\"\"\n",
    "    \n",
    "    # Создаем оконную функцию для нахождения наиболее частой станции в геозоне\n",
    "    window_spec = Window.partitionBy(geo_hash_col).orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Создаем DataFrame с наиболее частыми станциями\n",
    "    common_stations = df.filter(col(station_col).isNotNull()) \\\n",
    "        .groupBy(geo_hash_col, station_col) \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(geo_hash_col, col(station_col).alias(f\"common_{station_col}\"))\n",
    "    \n",
    "    # Присоединяем наиболее частые станции\n",
    "    df_filled = df.join(common_stations, on=geo_hash_col, how=\"left\") \\\n",
    "        .withColumn(f\"{station_col}_clean\", \n",
    "                   coalesce(col(station_col), col(f\"common_{station_col}\"))) \\\n",
    "        .drop(f\"common_{station_col}\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Заполняем пропущенные стартовые станции\n",
    "filtered_df = fill_missing_stations(filtered_df, \"start_station_name\", \"start_geo_hash\")\n",
    "\n",
    "# Заполняем пропущенные конечные станции\n",
    "filtered_df = fill_missing_stations(filtered_df, \"end_station_name\", \"end_geo_hash\")\n",
    "\n",
    "# Заменяем оригинальные колонки очищенными\n",
    "filtered_df = filtered_df \\\n",
    "    .drop(\"start_station_name\", \"end_station_name\") \\\n",
    "    .withColumnRenamed(\"start_station_name_clean\", \"start_station_name\") \\\n",
    "    .withColumnRenamed(\"end_station_name_clean\", \"end_station_name\") \\\n",
    "    .drop(\"start_geo_hash\", \"end_geo_hash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5872aefb-fa73-4ddf-9d2a-e56e2a6a2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Удаляем поездки с экстремально малым расстоянием при большой длительности\n",
    "filtered_df = filtered_df.filter(\n",
    "    ~((col(\"distance_km\") < 0.01) & (col(\"ride_duration_minutes\") > 40))\n",
    ")\n",
    "\n",
    "# 2. Фильтр для поездок на ту же станцию с аномальной длительностью\n",
    "filtered_df = filtered_df.filter(\n",
    "    ~(\n",
    "        (col(\"start_station_name\") == col(\"end_station_name\")) &\n",
    "        (col(\"ride_duration_minutes\") > 1440)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Удаляем поездки с нереальной скоростью\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"speed_kmh\",\n",
    "    when(col(\"ride_duration_minutes\") > 0,\n",
    "         col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0)\n",
    ")\n",
    "\n",
    "filtered_df = filtered_df.filter(\n",
    "    (col(\"speed_kmh\") <= 45) | (col(\"speed_kmh\").isNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b1ffc1-b189-40b0-aeda-3fd9fdd6bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_with_features = filtered_df \\\n",
    "    .withColumn(\"lat_grid\", round(col(\"start_lat\"), 2)) \\\n",
    "    .withColumn(\"lng_grid\", round(col(\"start_lng\"), 2)) \\\n",
    "    .withColumn(\"is_weekend\", \n",
    "                when((col(\"ride_dayofweek\") == 1) | \n",
    "                     (col(\"ride_dayofweek\") == 7), True).otherwise(False)) \\\n",
    "    .withColumn(\"lat_diff\", abs(col(\"end_lat\") - col(\"start_lat\"))) \\\n",
    "    .withColumn(\"lng_diff\", abs(col(\"end_lng\") - col(\"start_lng\"))) \\\n",
    "    .withColumn(\"coord_diff\", col(\"lat_diff\") + col(\"lng_diff\")) \\\n",
    "    .withColumn(\"speed_kmh\", \n",
    "                when(col(\"ride_duration_minutes\") > 0, \n",
    "                     col(\"distance_km\") / (col(\"ride_duration_minutes\") / 60.0)).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765e527-8bb0-4252-bb6a-0239a6ac6e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7412cef2-b515-4b08-a85d-c1c07771d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ОБЩАЯ СТАТИСТИКА:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o653.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 1 times, most recent failure: Lost task 30.0 in stage 1.0 (TID 91) (usatopolosato executor driver): java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Общая статистика\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. ОБЩАЯ СТАТИСТИКА:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m cleaned_df_with_features\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m      4\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВсего_записей\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mround\u001b[39m(mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСр_длит_мин\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМин_длит\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_duration_minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМакс_длит\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mround\u001b[39m(mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_km\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСр_расст_км\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance_km\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМакс_расст_км\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m )\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o653.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 1 times, most recent failure: Lost task 30.0 in stage 1.0 (TID 91) (usatopolosato executor driver): java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "# 1. Общая статистика\n",
    "print(\"\\n1. ОБЩАЯ СТАТИСТИКА:\")\n",
    "cleaned_df_with_features.select(\n",
    "    count(\"*\").alias(\"Всего_записей\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"Ср_длит_мин\"),\n",
    "    round(min(\"ride_duration_minutes\"), 2).alias(\"Мин_длит\"),\n",
    "    round(max(\"ride_duration_minutes\"), 2).alias(\"Макс_длит\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"Ср_расст_км\"),\n",
    "    round(max(\"distance_km\"), 3).alias(\"Макс_расст_км\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80248911-c4ee-4a10-bb96-509846ebcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. СТАТИСТИКА ПО ПОЛЬЗОВАТЕЛЯМ (member_casual):\")\n",
    "cleaned_df_with_features.groupBy(\"member_casual\").agg(\n",
    "    count(\"*\").alias(\"Поездок\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 1).alias(\"Доля_%\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"Ср_длит_мин\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"Ср_расст_км\")\n",
    ").orderBy(desc(\"Поездок\")).show()\n",
    "\n",
    "# 3. Статистика по типам велосипедов\n",
    "print(\"\\n3. СТАТИСТИКА ПО ТИПАМ ВЕЛОСИПЕДОВ (rideable_type):\")\n",
    "cleaned_df_with_features.groupBy(\"rideable_type\").agg(\n",
    "    count(\"*\").alias(\"Поездок\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 1).alias(\"Доля_%\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"Ср_длит_мин\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"Ср_расст_км\")\n",
    ").orderBy(desc(\"Поездок\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa73cb-8284-433f-b4ac-aecb5f5a5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. СТАТИСТИКА ПО ДНЯМ НЕДЕЛИ:\")\n",
    "cleaned_df_with_features.groupBy(\"ride_dayofweek\").agg(\n",
    "    count(\"*\").alias(\"Поездок\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 1).alias(\"Доля_%\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"Ср_длит_мин\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"Ср_расст_км\")\n",
    ").orderBy(\"ride_dayofweek\").show(7)\n",
    "\n",
    "# 5. Статистика по часам\n",
    "print(\"\\n5. СТАТИСТИКА ПО ЧАСАМ СУТОК:\")\n",
    "cleaned_df_with_features.groupBy(\"ride_hour\").agg(\n",
    "    count(\"*\").alias(\"Поездок\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 1).alias(\"Доля_%\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"Ср_длит_мин\")\n",
    ").orderBy(\"ride_hour\").show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591be5-5b3c-41f4-8e41-456c8d733f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Описательная статистика\n",
    "print(\"\\n6. ОПИСАТЕЛЬНАЯ СТАТИСТИКА (describe):\")\n",
    "cleaned_df_with_features.select(\n",
    "    \"ride_duration_minutes\", \"distance_km\", \"speed_kmh\"\n",
    ").describe().show()\n",
    "\n",
    "# 7. Квантили распределения\n",
    "print(\"\\n7. КВАНТИЛИ РАСПРЕДЕЛЕНИЯ ДЛИТЕЛЬНОСТИ:\")\n",
    "quantiles = cleaned_df_with_features.approxQuantile(\n",
    "    \"ride_duration_minutes\", \n",
    "    [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99], \n",
    "    0.01\n",
    ")\n",
    "\n",
    "# Создаем DataFrame для квантилей\n",
    "from pyspark.sql import Row\n",
    "quantile_data = [(f\"{p*100:.0f}%\", v) for p, v in zip(\n",
    "    [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99], \n",
    "    quantiles\n",
    ")]\n",
    "\n",
    "quantile_df = spark.createDataFrame(\n",
    "    [Row(Квантиль=k, Длительность_мин=v) for k, v in quantile_data]\n",
    ")\n",
    "quantile_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f97ac-52eb-4ec1-a932-abba87ddd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Географическая статистика\n",
    "print(\"\\n8. ГЕОГРАФИЧЕСКАЯ СТАТИСТИКА:\")\n",
    "cleaned_df_with_features.select(\n",
    "    round(min(\"start_lat\"), 4).alias(\"Мин_широта\"),\n",
    "    round(max(\"start_lat\"), 4).alias(\"Макс_широта\"),\n",
    "    round(mean(\"start_lat\"), 4).alias(\"Ср_широта\"),\n",
    "    round(min(\"start_lng\"), 4).alias(\"Мин_долгота\"),\n",
    "    round(max(\"start_lng\"), 4).alias(\"Макс_долгота\"),\n",
    "    round(mean(\"start_lng\"), 4).alias(\"Ср_долгота\"),\n",
    "    countDistinct(\"start_station_name\").alias(\"Уник_старт_станций\"),\n",
    "    countDistinct(\"end_station_name\").alias(\"Уник_конеч_станций\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cdf58-913c-4a36-8156-f71c15a4fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Статистика по скорости\n",
    "print(\"\\n9. СТАТИСТИКА ПО СКОРОСТИ:\")\n",
    "cleaned_df_with_features.select(\n",
    "    round(mean(\"speed_kmh\"), 1).alias(\"Ср_скорость_кмч\"),\n",
    "    round(stddev(\"speed_kmh\"), 1).alias(\"Стд_отклонение\"),\n",
    "    round(min(\"speed_kmh\"), 1).alias(\"Мин_скорость\"),\n",
    "    round(max(\"speed_kmh\"), 1).alias(\"Макс_скорость\"),\n",
    "    count(when(col(\"speed_kmh\") > 25, True)).alias(\"Поездок_25кмч\"),\n",
    "    count(when(col(\"speed_kmh\") > 30, True)).alias(\"Поездок_30кмч\")\n",
    ").show()\n",
    "\n",
    "# 10. Статистика по возвратам на станцию\n",
    "print(\"\\n10. СТАТИСТИКА ПО ВОЗВРАТАМ:\")\n",
    "cleaned_df_with_features.select(\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"Процент_возвратов_%\"),\n",
    "    round(mean(when(col(\"same_station\") == True, col(\"ride_duration_minutes\"))), 1).alias(\"Ср_длит_возврат_мин\"),\n",
    "    round(mean(when(col(\"same_station\") == False, col(\"ride_duration_minutes\"))), 1).alias(\"Ср_длит_перемещение_мин\"),\n",
    "    round(mean(when(col(\"same_station\") == True, col(\"distance_km\"))), 2).alias(\"Ср_расст_возврат_км\"),\n",
    "    round(mean(when(col(\"same_station\") == False, col(\"distance_km\"))), 2).alias(\"Ср_расст_перемещение_км\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113cb21-0314-4a70-8f7f-916ab1252f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Временной диапазон\n",
    "print(\"\\n11. ВРЕМЕННОЙ ДИАПАЗОН ДАННЫХ:\")\n",
    "cleaned_df_with_features.select(\n",
    "    min(\"started_at\").alias(\"Первая_поездка\"),\n",
    "    max(\"started_at\").alias(\"Последняя_поездка\"),\n",
    "    datediff(max(\"started_at\"), min(\"started_at\")).alias(\"Дней_в_данных\"),\n",
    "    countDistinct(\"ride_date\").alias(\"Уник_дней\")\n",
    ").show()\n",
    "\n",
    "# 12. Топ-10 популярных стартовых станций\n",
    "print(\"\\n12. ТОП-10 ПОПУЛЯРНЫХ СТАРТОВЫХ СТАНЦИЙ:\")\n",
    "cleaned_df_with_features.groupBy(\"start_station_name\").agg(\n",
    "    count(\"*\").alias(\"Поездок\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"Ср_длит_мин\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"Ср_расст_км\")\n",
    ").orderBy(desc(\"Поездок\")).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb21eea8-f0ee-4051-889a-0d0b6c2b0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ride_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- started_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- start_lat: double (nullable = true)\n",
      " |-- start_lng: double (nullable = true)\n",
      " |-- end_lat: double (nullable = true)\n",
      " |-- end_lng: double (nullable = true)\n",
      " |-- member_casual: string (nullable = true)\n",
      " |-- ride_date: date (nullable = true)\n",
      " |-- ride_year: integer (nullable = true)\n",
      " |-- ride_month: integer (nullable = true)\n",
      " |-- ride_day: integer (nullable = true)\n",
      " |-- ride_dayofweek: integer (nullable = true)\n",
      " |-- ride_hour: integer (nullable = true)\n",
      " |-- ride_duration_seconds: long (nullable = true)\n",
      " |-- ride_duration_minutes: double (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- lat_grid: double (nullable = true)\n",
      " |-- lng_grid: double (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = false)\n",
      " |-- lat_diff: double (nullable = true)\n",
      " |-- lng_diff: double (nullable = true)\n",
      " |-- coord_diff: double (nullable = true)\n",
      " |-- same_station: boolean (nullable = false)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- speed_kmh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df_with_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be43b27-bbd0-4945-92cb-4f87a8d4856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СОЗДАНИЕ ВИТРИН ДАННЫХ ДЛА UNIT-ЭКОНОМИКИ...\n",
      "\n",
      "1. ВИТРИНА: Основные метрики поездок\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o688.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 5.0 failed 1 times, most recent failure: Lost task 16.0 in stage 5.0 (TID 122) (usatopolosato executor driver): java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m      7\u001b[0m ride_metrics \u001b[38;5;241m=\u001b[39m cleaned_df_with_features\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mride_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmember_casual\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_station\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_return_trip\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Возврат = экономия на перебалансировке\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Сохраняем\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m ride_metrics\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ride_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСохранено в ride_metrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Витрина 2: Пользовательская активность (LTV расчет)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:2146\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   2129\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2130\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2144\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   2145\u001b[0m )\n\u001b[1;32m-> 2146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o688.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 5.0 failed 1 times, most recent failure: Lost task 16.0 in stage 5.0 (TID 122) (usatopolosato executor driver): java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Недостаточно места на диске\r\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\r\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\r\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\r\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\r\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\r\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:540)\r\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:70)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:337)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:174)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "print(\"СОЗДАНИЕ ВИТРИН ДАННЫХ ДЛА UNIT-ЭКОНОМИКИ...\")\n",
    "\n",
    "# Витрина 1: Основные метрики поездок (ядро unit-экономики)\n",
    "print(\"\\n1. ВИТРИНА: Основные метрики поездок\")\n",
    "ride_metrics = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"member_casual\",\n",
    "    \"rideable_type\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    \"same_station\",\n",
    "    \"start_station_name\",\n",
    "    \"end_station_name\",\n",
    "    col(\"ride_duration_minutes\").alias(\"unit_cost_driver\"),  # Длительность как драйвер износа\n",
    "    col(\"distance_km\").alias(\"unit_distance_driver\"),  # Расстояние как драйвер износа\n",
    "    when(col(\"same_station\") == True, 1).otherwise(0).alias(\"is_return_trip\")  # Возврат = экономия на перебалансировке\n",
    ")\n",
    "\n",
    "# Сохраняем\n",
    "ride_metrics.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/ride_metrics.csv\")\n",
    "print(f\"Сохранено в ride_metrics.csv\")\n",
    "\n",
    "# Витрина 2: Пользовательская активность (LTV расчет)\n",
    "print(\"\\n2. ВИТРИНА: Пользовательская активность (для LTV)\")\n",
    "user_activity = cleaned_df_with_features.groupBy(\"member_casual\").agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    countDistinct(\"ride_id\").alias(\"unique_users_estimate\"),  # Предполагаем ride_id уникален\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"avg_ride_duration\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"avg_ride_distance\"),\n",
    "    round(stddev(\"ride_duration_minutes\"), 2).alias(\"std_ride_duration\"),\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 1).alias(\"total_usage_hours\"),  # Часы использования\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ")\n",
    "\n",
    "user_activity.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/user_activity.csv\")\n",
    "print(f\"Сохранено в user_activity.csv\")\n",
    "\n",
    "# Витрина 3: Использование велосипедов (амортизация)\n",
    "print(\"\\n3. ВИТРИНА: Использование велосипедов (амортизация)\")\n",
    "bike_utilization = cleaned_df_with_features.groupBy(\"rideable_type\").agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    round(count(\"*\") * 100.0 / cleaned_df_with_features.count(), 2).alias(\"market_share_percent\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 2).alias(\"avg_usage_time_min\"),\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 1).alias(\"total_usage_hours\"),\n",
    "    round(mean(\"distance_km\"), 3).alias(\"avg_distance_per_ride\"),\n",
    "    round(sum(\"distance_km\"), 1).alias(\"total_distance_km\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ").orderBy(desc(\"total_rides\"))\n",
    "\n",
    "bike_utilization.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/bike_utilization.csv\")\n",
    "print(f\"Сохранено в bike_utilization.csv\")\n",
    "\n",
    "# Витрина 4: Временные паттерны (сезонность и нагрузка)\n",
    "print(\"\\n4. ВИТРИНА: Временные паттерны (для прогнозирования спроса)\")\n",
    "time_patterns = cleaned_df_with_features.groupBy(\n",
    "    \"ride_year\", \"ride_month\", \"ride_dayofweek\", \"ride_hour\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"ride_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\"),\n",
    "    round(mean(when(col(\"is_weekend\") == True, 1).otherwise(0)) * 100, 1).alias(\"weekend_percent\")\n",
    ").orderBy(\"ride_year\", \"ride_month\", \"ride_dayofweek\", \"ride_hour\")\n",
    "\n",
    "time_patterns.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/time_patterns.csv\")\n",
    "print(f\"Сохранено в time_patterns.csv\")\n",
    "\n",
    "# Витрина 5: Географическая активность (распределение станций)\n",
    "print(\"\\n5. ВИТРИНА: Географическая активность (оптимизация станций)\")\n",
    "geo_activity = cleaned_df_with_features.groupBy(\n",
    "    \"start_station_name\", \"end_station_name\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"route_frequency\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(stddev(\"ride_duration_minutes\"), 1).alias(\"std_duration\"),\n",
    "    round(sum(when(col(\"member_casual\") == \"member\", 1).otherwise(0))).alias(\"member_count\"),\n",
    "    round(sum(when(col(\"member_casual\") == \"casual\", 1).otherwise(0))).alias(\"casual_count\")\n",
    ").filter(col(\"route_frequency\") >= 10)  # Только популярные маршруты\n",
    "\n",
    "geo_activity.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/geo_activity.csv\")\n",
    "print(f\"Сохранено в geo_activity.csv\")\n",
    "\n",
    "# Витрина 6: Станционная экономика\n",
    "print(\"\\n6. ВИТРИНА: Станционная экономика\")\n",
    "station_economics = cleaned_df_with_features.groupBy(\"start_station_name\").agg(\n",
    "    count(\"*\").alias(\"departures_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_departure_duration\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_departure_distance\"),\n",
    "    \n",
    "    # Конечные станции\n",
    "    countDistinct(\"end_station_name\").alias(\"unique_destinations\"),\n",
    "    \n",
    "    # Возвраты\n",
    "    round(sum(when(col(\"same_station\") == True, 1).otherwise(0))).alias(\"return_count\"),\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\")\n",
    ").filter(col(\"departures_count\") >= 5).orderBy(desc(\"departures_count\"))\n",
    "\n",
    "station_economics.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/station_economics.csv\")\n",
    "print(f\"Сохранено в station_economics.csv\")\n",
    "\n",
    "# Витрина 7: Метрики скорости (риски и безопасность)\n",
    "print(\"\\n7. ВИТРИНА: Метрики скорости (рисковые поездки)\")\n",
    "speed_metrics = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"member_casual\",\n",
    "    \"rideable_type\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    when(col(\"speed_kmh\") > 25, \"high_speed\").when(col(\"speed_kmh\") > 15, \"medium_speed\").otherwise(\"low_speed\").alias(\"speed_category\"),\n",
    "    when(col(\"ride_duration_minutes\") > 120, \"long_ride\").when(col(\"ride_duration_minutes\") > 30, \"medium_ride\").otherwise(\"short_ride\").alias(\"duration_category\")\n",
    ").filter(col(\"speed_kmh\") > 0)\n",
    "\n",
    "speed_metrics.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/speed_metrics.csv\")\n",
    "print(f\"Сохранено в speed_metrics.csv\")\n",
    "\n",
    "# Витрина 8: Экономика возвратов (экономия на перебалансировке)\n",
    "print(\"\\n8. ВИТРИНА: Экономика возвратов\")\n",
    "return_economics = cleaned_df_with_features.groupBy(\n",
    "    \"start_station_name\", \n",
    "    col(\"same_station\").alias(\"is_return_trip\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"trip_count\"),\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_distance_km\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\")\n",
    ").orderBy(\"start_station_name\", desc(\"is_return_trip\"))\n",
    "\n",
    "return_economics.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/return_economics.csv\")\n",
    "print(f\"Сохранено в return_economics.csv\")\n",
    "\n",
    "# Витрина 9: Агрегированные KPI для дашборда\n",
    "print(\"\\n9. ВИТРИНА: Агрегированные KPI\")\n",
    "aggregated_kpi = cleaned_df_with_features.agg(\n",
    "    count(\"*\").alias(\"total_rides\"),\n",
    "    \n",
    "    # Пользователи\n",
    "    round(mean(when(col(\"member_casual\") == \"member\", 1).otherwise(0)) * 100, 1).alias(\"member_percent\"),\n",
    "    round(mean(when(col(\"member_casual\") == \"casual\", 1).otherwise(0)) * 100, 1).alias(\"casual_percent\"),\n",
    "    \n",
    "    # Unit-метрики\n",
    "    round(mean(\"ride_duration_minutes\"), 1).alias(\"avg_ride_duration_min\"),\n",
    "    round(mean(\"distance_km\"), 2).alias(\"avg_ride_distance_km\"),\n",
    "    round(mean(\"speed_kmh\"), 1).alias(\"avg_speed_kmh\"),\n",
    "    \n",
    "    # Экономика\n",
    "    round(mean(when(col(\"same_station\") == True, 1).otherwise(0)) * 100, 1).alias(\"return_rate_percent\"),\n",
    "    \n",
    "    # Время\n",
    "    round(sum(\"ride_duration_minutes\") / 60, 0).alias(\"total_usage_hours\"),\n",
    "    \n",
    "    # География\n",
    "    countDistinct(\"start_station_name\").alias(\"unique_start_stations\"),\n",
    "    countDistinct(\"end_station_name\").alias(\"unique_end_stations\")\n",
    ")\n",
    "\n",
    "# Создаем одну строку с KPI\n",
    "aggregated_kpi.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/aggregated_kpi.csv\")\n",
    "print(\"Сохранено в aggregated_kpi.csv\")\n",
    "\n",
    "# Витрина 10: Детализированные данные для анализа\n",
    "print(\"\\n10. ВИТРИНА: Детализированные данные\")\n",
    "detailed_data = cleaned_df_with_features.select(\n",
    "    \"ride_id\",\n",
    "    \"rideable_type\",\n",
    "    \"member_casual\",\n",
    "    \"started_at\",\n",
    "    \"ended_at\",\n",
    "    \"ride_duration_minutes\",\n",
    "    \"distance_km\",\n",
    "    \"speed_kmh\",\n",
    "    \"same_station\",\n",
    "    \"is_weekend\",\n",
    "    \"ride_dayofweek\",\n",
    "    \"ride_hour\",\n",
    "    \"start_station_name\",\n",
    "    \"end_station_name\",\n",
    "    \"start_lat\",\n",
    "    \"start_lng\",\n",
    "    \"end_lat\",\n",
    "    \"end_lng\",\n",
    "    # Добавляем бизнес-метрики\n",
    "    (col(\"ride_duration_minutes\") * 0.1).alias(\"estimated_wear_cost\"),  # Предполагаемая стоимость износа\n",
    "    when(col(\"same_station\") == True, 5.0).otherwise(0.0).alias(\"estimated_rebalancing_saving\")  # Экономия на перебалансировке\n",
    ")\n",
    "\n",
    "detailed_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/detailed_data.csv\")\n",
    "print(f\"Сохранено в detailed_data.csv\")\n",
    "\n",
    "# Сохраняем оригинальные данные тоже\n",
    "print(\"\\n11. СОХРАНЕНИЕ ОРИГИНАЛЬНЫХ ДАННЫХ...\")\n",
    "cleaned_df_with_features.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"data/full_dataset.csv\")\n",
    "print(f\"Сохранено в full_dataset.csv\")\n",
    "\n",
    "print(\"ВСЕ ВИТРИНЫ СОХРАНЕНЫ УСПЕШНО!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79bae48d-66c8-4967-b507-5d1d5fa5254f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05611c2-a0d6-4540-a8c2-a29838d37809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
